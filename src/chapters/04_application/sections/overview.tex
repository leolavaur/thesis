\section{A Practical Use Case for FIDSs\label{sec:app.overview}}
% organizations
% sake of simplicity -> hfl, same features
% not unrealistic, 
% - eg. same probe deployed in multiple organizations
% - gray-box product that all organizations use

We consider a typical \gls{fl} scenario where a central server $S$ is tasked with aggregating the model updates $w_k^r$ of a set of participants $P = \lbrace p_k | k\in \llbracket 1,n \rrbracket\rbrace$ at each round $r$.
The participants $p_k$ are entities that oversee an organization's network, which makes them highly available and interested.
This can be described as a \gls{csfl} scenario, \ie, fewer participants with consequent amounts of data and significant computing capabilities.
Because of the lower scale of the federation and the assumed interest of the different parties, we set the fraction $C$ of participants that are selected at each round to $1$.

For the sake of simplicity, we consider that all participants share the same model architecture and extract the same features from the network traffic.
This is not unrealistic, as common formats and protocols are used in the industry for this purpose, such as Cisco's NetFlow format~\cite{rfc3954} for network flows.
Further, this description can fit multiple scenarios, such as organizations deploying the same probe in their network as part of a standardization effort, or a service provider offering a gray-box product to multiple organizations.
Although the features are assumed to be identical across participants, the distribution of the data can vary considerably, as each organization has its own network configuration and security policies~\cite{zhou_surveycoordinatedattacks_2010}.

We also consider that participants have access to labeled data, which is a common assumption in the literature.
Although labeling data can be costly, it is a more reasonable assumption in \gls{csfl} scenarios, where participants are more likely to have the human and financial resources to label data.
Therefore, each participant possesses a local dataset $d_k = (\mathcal{X}_k, \mathcal{Y}_k)$ that is not shared with the others.
Because of the differences between organizations, the distribution of each local dataset $d_k$ can vary considerably, independently of the associated labels.
Indeed, the same network behavior (say \gls{p2p} file sharing) might be considered normal in an organization (\eg, a media company) but flagged as suspicious or outright malicious in another (\eg, a financial institution).
However, the \gls{cids} use case implies that similarities can exist between participants, for instance between organizations operating in the same sector or having similar network infrastructure.
This particular setting can be described as \emph{practical} \gls{niid}, as opposed to the \emph{pathological} \gls{niid} settings, where all participants have unique and highly different data-distributions~\cite{huang_PersonalizedCrossSiloFederated_2021}.
This is the most common setting in \glspl{fids}, as it serves the goal of improving behavior characterization, and having access to knowledge that cannot be inferred with only local data.


\subsection{Dataset selection\label{sec:app.overview.dataset}}

Since we consider that all organizations share the same model architecture, we need multiple independently-generated datasets that share the same feature set.
Fortunately, \textcite{sarhan_StandardFeatureSet_2022} have proposed a standard feature set for \gls{ids} datasets, based on NetFlow v9 (see \Cref{sec:bg.ids.datasets}).
Namely, we used the modified versions of the following datasets:
\begin{itemize}
    \item UNSW-NB15~\cite{moustafa_UNSWNB15comprehensivedata_2015} is produced using the IXIA PerfectStorm tool on the Cyber Range Lab of UNSW Canberra.
    The traffic is a hybrid set of real modern normal activities and synthetic contemporary attack behaviors, grouped in 9 attack classes.
    \item Bot-IoT~\cite{koroniotis_developmentrealisticbotnet_2019} is another dataset generated at USNW, using a realistic smart home environment setup, completed by IoT devices.
    It focuses on the detection of IoT botnet attacks, the DoS and DDoS classes being the most represented.
    %Other classes of attacks include OS and Service Scan, Keylogging and Data exfiltration.
    This dataset is highly unbalanced, as the majority of the traffic is malicious.
    \item ToN\_IoT~\cite{moustafa_FederatedTON_IoTWindows_2020} is yet another dataset generated by the same team, containing IoT/IIoT telemetry data, network traffic, as well as system logs.
    The network dataset contains 9 attack classes, including Ransomware, Scanning, and XSS. %: Backdoors, DoS, DDoS, Injection, MitM, Password, Ransomware, Scanning, and XSS.
    \item CSE-CIC-IDS2018~\cite{sharafaldin_GeneratingNewIntrusion_2018} is a dataset generated by the Canadian Institute for Cybersecurity in collaboration with the Communications Security Establishment (CSE).
    The traffic is collected on a large-scale infrastructure deployed on AWS.
    It contains 14 attack labels, grouped in 6 attack classes. %, namely Brute Force, Bot, DoS, DDoS, Infiltration, Web attacks.
\end{itemize}

In most of the experiments presented in this manuscript, We use the ``sampled'' version (1,000,000 data points per dataset) provided by the same team~\cite{layeghy_GeneralisabilityMachineLearningbased_2022}
We remove the port and IP addresses for both source and destination, as they are rather a representation of the network topology and device configurations than of traffic patterns~\cite{decarvalhobertoli_Generalizingintrusiondetection_2023}.
We then use one-hot encoding (see \Cref{sec:bg.ids.dl}) on the categorical features (both in the sample and labels), and apply min-max normalization to give all features the same importance in model training.
This pre-processing step produces 39 features for each sample.