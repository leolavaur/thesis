\section{Related Works\label{sec:radar.related}}

In the context of Byzantine-robust \gls{fl}, \texttt{FLAME}~\cite{nguyen_FLAMETamingBackdoors_2022} represents one of the most complete solutions, leveraging clustering, norm-clipping, and adaptive noising to mitigate attacks.
Yet, because it works under the assumption that the biggest cluster represents benign participants and that attackers cannot exceed 50\% of the population, \texttt{FLAME} \emph{de facto} falters against a majority of malicious clients.
Additionally, while \texttt{FLAME} can manage scenarios with low proportions of \gls{niid} participants, it aims to produce a single global model, which may not be optimal in highly skewed \gls{niid} environments where multiple sub-federations are needed.
A more comparabile approach leveraging clustering is proposed by \textcite{ye_PFedSAPersonalizedFederated_2023}, who use cosine similarity to group participants in more homogeneous subgroups.
However, as this approach doesn't aim to address Byzantines, it does not consider that some malicious participants might aim to be grouped with benign ones to poison the cluster's model.

\texttt{FoolsGold}~\cite{fung_LimitationsFederatedLearning_2020} and \texttt{CONTRA}~\cite{awan_CONTRADefendingPoisoning_2021} provide alternative strategies by identifying sybils through the similarity of model updates.
These methods are particularly well suited for \emph{pathological} \gls{niid} scenarios, as they assume sybils generate similar updates, thus distinguishing them from benign participants.
However, they struggle to detect lone attackers and face challenges in \emph{practical} \gls{niid} settings where legitimate communities of similar participants exist (\cf \Cref{sec:bg.fl.data}).
In such cases, these legitimate participants may be falsely classified as sybils, reducing the effectiveness of these strategies.

To overcome relying on a single source of truth without using similarity metrics, \textcite{zhao_ShieldingCollaborativeLearning_2020} leverage client-side evaluations to assess model quality.
To address \gls{niid} settings, clients self-report the labels on which they have sufficient data to evaluate the model, which introduces the risk of abusive self-reporting.
Despite its significant overhead in terms of resource and bandwidth consumption, this approach is remains appropriate for cross-silo \gls{fl} scenarios, which involves fewer participants with more data and dedicated resources.
A reputation system for \gls{fl} that leverages indirect feedback would be able to mitigate improper feedbacks and provide a more nuanced evaluation of participants' contributions.
However, to the best of our knowledge, none of the trust-scoring systems proposed in the literature fit the definition of a reputation system as posed in \Cref{sec:radar.prelim.reputation}.