Collaboration between different cybersecurity actors is essential to fight against increasingly sophisticated and numerous attacks.
However, stakeholders are often reluctant to share their data, fearing confidentiality and privacy issues and the loss of their competitive advantage, although it would improve their intrusion detection models.
Federated learning is a recent paradigm in machine learning that allows distributed clients to train a common model without sharing their data.
These properties of collaboration and confidentiality make it an ideal candidate for sensitive applications such as intrusion detection.
While several applications have shown that it is indeed possible to train a single model on distributed intrusion detection data, few have focused on the collaborative aspect of this paradigm.
%In addition to the collaborative aspect, other challenges arise in this context, such as the heterogeneity of the data between different participants or the management of untrusted contributions.
%
In this manuscript, we study the use of federated learning to build collaborative intrusion detection systems.
In particular, we explore
\begin{enumerate*}[label={\small(\roman*)}]
  \item the impact of data quality in heterogeneous contexts,
  \item the exposure to certain types of poisoning attacks, and
  \item tools and methodologies to improve the evaluation of these types of algorithms.
\end{enumerate*}