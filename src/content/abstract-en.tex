Collaboration between different cybersecurity actors is essential to fight against increasingly sophisticated and numerous attacks.
However, stakeholders are often reluctant to share their data, fearing confidentiality and privacy issues, although it would improve their intrusion detection models.
Federated learning is a recent paradigm in machine learning that allows distributed clients to train a common model without sharing their data.
These properties of collaboration and confidentiality make it an ideal candidate for sensitive applications such as intrusion detection.
While several applications have shown that it is indeed possible to train a single model on intrusion detection data, few have focused on the collaborative aspect of this paradigm.
In addition to the collaborative aspect, other challenges arise in this context, such as the heterogeneity of the data between different participants or the management of untrusted contributions.
%
In this manuscript, we explore the use of federated learning to build collaborative intrusion detection systems.
In particular, we explore the impact of data quality in heterogeneous contexts, some types of poisoning attacks, and propose tools and methodologies to improve the evaluation of these types of distributed algorithms.