% LTeX: enabled=false

\subsubsection{Costs and metrics}
\label{sec:sota.quali.metrics}

Researchers use metrics in the literature to assess, validate, and compare their solutions.
In this work, metrics are divided in three categories that follow the life cycle of \glspl{fids}: training, federation, and execution.

While training \gls{dl} models, most \gls{ml} frameworks use and display training loss and training accuracy, that are used to adapt the model's weights at each epoch.
When plotted on a time-based frame (time, epochs, or rounds in case of \gls{fl}), these metrics show the evolution of the model's training.



It can be used to measure the convergence time of the model, often characterized as obtaining an accuracy above a defined threshold (\eg 90\% in \cite{chen_Networkanomalydetection_2020b}), or with the percentage of loss improvement between two epochs (\eg 0.01 in \cite{kim_CollaborativeAnomalyDetection_2020}). Training time also serve as a comparison between approaches~\cite{schneble_Attackdetectionusing_2019}, even though it depends a lot on the underlying hardware architecture. Finally, it can be used as a metric to select other hyper-parameters, such as the number of epochs in \cite{liu_BlockchainFederatedLearning_2021}.

\textcite{li_DeepFedFederatedDeep_2020b} summed training time and communication time at each round in a unique metric \emph{time cost}.
This overall training cost is justified by their constrained environment, where resources are very limited.
Therefore, instead of reaching for maximum accuracy, the authors fix the accuracy as a target and iterate on hyper- and meta-parameters (\eg learning rate, epochs per round) to find the lowest \emph{time cost}.


Algorithm complexity and resource consumption are also relevant metrics to measure local training costs.
Constrained use cases like \gls{iot} require complex algorithm to run on resource-limited devices.
In \cite{pahl_AllEyesYou_2018}, the authors also study complexity to choose \gls{birch} clusters instead of K-means, as updating the former is easier---\(\mathcal{O}(d)\) vs \(\mathcal{O}(n*d)\), where \(d\) is the dataset size.

Hardware-related resources are used by \cite{rathore_BlockSecIoTNetBlockchainbaseddecentralized_2019,zhao_MultiTaskNetworkAnomaly_2019}, mostly to emphasize differences between their approach and another, often more standard one.
These resources often include CPU, disk and memory usage, as well as energy consumption.
However, evaluating hardware-related metrics requires experiments to be implemented using the same hardware and software stacks.
Hardware- and energy-based metrics are especially relevant in constrained scenarios~\cite{nguyen_DIoTFederatedSelflearning_2019,schneble_Attackdetectionusing_2019}, whereas training time is relevant for most use case, while not a priority.
When these measures are collected on reference hardware, it can also be used to evaluate the feasibility of the approach, as in \cite{nguyen_DIoTFederatedSelflearning_2019}, if the hardware matches the deployment constraints of the study.




Federation-related metrics are heavily tied to the communication between clients, or with a server.
The communication overhead is a core metric of \glspl{fids}, as high bandwidth consumption is a drawback of \glspl{cids} (see \Cref{chall:latency}), especially in constrained environments~\cite{qin_LineSpeedScalableIntrusion_2020a}.
The overhead is often measured in bytes, either per packets~\cite{pahl_AllEyesYou_2018}, or for the total of all communications~\cite{schneble_Attackdetectionusing_2019,zhang_BlockchainbasedFederatedLearning_2020}.

Metrics must be adapted to the specificities of each solution, for instance when adding a feature.
Consequently, \textcite{zhang_BlockchainbasedFederatedLearning_2020} add specific metrics in their evaluation to measure the impact of using the blockchain, like the time of the \emph{blockchain encoring process}.
Some works \cite{rathore_BlockSecIoTNetBlockchainbaseddecentralized_2019,li_DeepFedFederatedDeep_2020,chen_Networkanomalydetection_2020,fan_IoTDefenderFederatedTransfer_2020,rahman_InternetThingsIntrusion_2020,Sun2020,al-athbaal-marri_FederatedMimicLearning_2020,Popoola2021}, on the other hand, do not cover federation-related metrics in their evaluation, which is questionable as it is a critical part of \glspl{fids}.

Finally, execution-related metrics are mostly focused on performance, and often come from the \gls{ml} community.
As shown in \Cref{tbl:selected.perf}, \emph{accuracy} is used by almost all reviewed works, followed by \emph{precision} and \emph{recall}.
Researchers often use accuracy to compare their results with related works.
Accuracy can be completed by \emph{fallout}, \emph{specificity}, and \emph{miss rate} (\Cref{sec:background.metrics}).


More performance metrics can be derived from these, like \emph{F1-score}, or the \gls{auc}.
The latter is obtained from the \gls{roc} curve, which is used to evaluate binary classification algorithms~\cite{pahl_AllEyesYou_2018,rathore_BlockSecIoTNetBlockchainbaseddecentralized_2019,nguyen_DIoTFederatedSelflearning_2019}.
\Gls{mcc} is another popular metric for binary classification tasks, and considered by some as the best metric for this use case~\cite{Chicco2020}.
It is used in \cite{rathore_BlockSecIoTNetBlockchainbaseddecentralized_2019}.
Finally, the confusion matrix is used by \cite{zhao_MultiTaskNetworkAnomaly_2019,al-athbaal-marri_FederatedMimicLearning_2020,chen_Networkanomalydetection_2020b,Sun2021}.
It allows a visualization of classification performance by opposing predicted classes against the real ones.
While critical in intrusion detection tasks, the miss rate is never directly addressed by the selected works, as the author often prefer the related \emph{recall} metric.

Other execution metrics like execution time are considered, as it can be critical for intrusion detection tasks.
Latency allows a comparison between different architectures, especially \emph{centralized}, \emph{distributed}, and \emph{decentralized}~\cite{rathore_BlockSecIoTNetBlockchainbaseddecentralized_2019}.
Latency is also relevant for highly constrained setups, as in \cite{qin_LineSpeedScalableIntrusion_2020a}.
As pointed out in \Cref{sec:sota.quali.location}, \emph{\gls{ml} location} can have an impact on data collection, but also on detection latency, if data need to travel over network to be analyzed.

Execution metrics are only relevant when comparing works that share implementation.
Such comparison is often performed by reimplementing a selection of related works.
They can also be used to highlight differences between approaches, like between \emph{local}, \emph{federated}, and \emph{ideal} models~\cite{li_DeepFedFederatedDeep_2020,rathore_BlockSecIoTNetBlockchainbaseddecentralized_2019}.